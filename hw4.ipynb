{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hiennoob123/Simple-game-for-life/blob/main/hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem is based on the official gym environment provided in https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "\n",
        "Since the official version has continuous state space and continuous action space, which requires new techniques to handle that is out side the scope of this course, we discretize the state space and action space. The discretized version is provided in the following package."
      ],
      "metadata": {
        "id": "FA5uq8nEacY0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMmN4EX2QSRd"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaist-or-gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code creates a pendulum environment."
      ],
      "metadata": {
        "id": "82tw8xDRazjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import kaist_or_gym\n",
        "from kaist_or_gym.envs.pendulum import DiscretePendulumEnv\n",
        "\n",
        "env = DiscretePendulumEnv(render_mode=\"human\")"
      ],
      "metadata": {
        "id": "BLtbf9qAQVzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code to visualize an episode generated by a random policy."
      ],
      "metadata": {
        "id": "xrBqHbWia4kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "obs, info = env.reset()\n",
        "for step in range(200):\n",
        "    # Sample a random action (uniform over 4 directions)\n",
        "    action = env.action_space.sample()\n",
        "    # Perform the action\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    env.render()\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(f\"Reached goal at step {step}.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "QZsaSJC0Qi7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reward is designed so that the inverted (upright) position of the pendulum is the desired state. See https://gymnasium.farama.org/environments/classic_control/pendulum/ for the full description of the reward function."
      ],
      "metadata": {
        "id": "_5OULNHDjIBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a function introduced in lecture that runs a single value iteration step using probability matrix P and current value function Q and discounting factor gamma."
      ],
      "metadata": {
        "id": "tOdfjINjbHG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def single_value_iteration(P, Q, gamma):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    - P: A dictionary mapping (s, a) → {next_state: probability}.\n",
        "        If (s, a) is missing, the next state is assumed to be drawn uniformly\n",
        "        from all states.\n",
        "    - Q: Current Q-value matrix of shape (n_states, n_actions).\n",
        "    - gamma: Discount factor.\n",
        "    \"\"\"\n",
        "\n",
        "    Q_new = np.zeros(Q.shape, dtype=float)\n",
        "    V = Q.max(axis=1)\n",
        "    for s in range(n_states):\n",
        "        for a in range(n_actions):\n",
        "            if (s, a) not in P:\n",
        "                Q_new[s, a] = env.reward(s, a) + gamma * np.mean(V)\n",
        "            else:\n",
        "                Q_new[s, a] = env.reward(s, a)\n",
        "                for next_state, prob in P[s, a].items():\n",
        "                    Q_new[s, a] += prob * gamma * V[next_state]\n",
        "    return Q_new\n"
      ],
      "metadata": {
        "id": "gEXF0Rq2Smo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a function introduced in lecture that computes empirical (estimated) transition probability given a dataset of transitions."
      ],
      "metadata": {
        "id": "GHjz93C6bPTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_empirical_transition_probabilities(D):\n",
        "    counts = {}  # (s, a) -> {s': count}\n",
        "    for s, a, r, s_prime in D:\n",
        "        if (s, a) not in counts:\n",
        "            counts[(s, a)] = {}\n",
        "        counts[(s, a)][s_prime] = counts[(s, a)].get(s_prime, 0) + 1\n",
        "\n",
        "    empirical_probs = {}  # (s, a) -> {s': prob}\n",
        "    for (s, a), s_prime_counts in counts.items():\n",
        "        total_transitions = sum(s_prime_counts.values())\n",
        "        if total_transitions > 0:\n",
        "            empirical_probs[(s, a)] = {s_prime: count / total_transitions for s_prime, count in s_prime_counts.items()}\n",
        "    return empirical_probs"
      ],
      "metadata": {
        "id": "Kb3Ak5uiSiBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function runs an episode using a policy (optionally with noise for ε-greedy exploration) and returns a list of transitions."
      ],
      "metadata": {
        "id": "yQeGxylHbXUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_one_episode(env, H: int, policy=None, noise=0):\n",
        "    D = []\n",
        "    s, _ = env.reset()\n",
        "    for _ in range(H):\n",
        "        if policy is None or np.random.rand() < noise:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            a = policy[s]\n",
        "        s_prime, reward, terminated, truncated, _ = env.step(a)\n",
        "        D.append((s, a, reward, s_prime))\n",
        "        s = s_prime\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return D"
      ],
      "metadata": {
        "id": "DQKh3PvoSsFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a skeleton code introduced in lecture that runs value iteration. Please complete the code."
      ],
      "metadata": {
        "id": "8g-zfDAWbl3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# you may have to tweak these\n",
        "K = 50 # number of episodes\n",
        "H = 50 # episode length\n",
        "\n",
        "# compute epsilon to be used in epsilon greedy method\n",
        "# you may have to tweak this function.\n",
        "def get_epsilon(k):\n",
        "    return 0.5 ** k\n",
        "\n",
        "gamma = 0.99\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "Q = np.zeros((n_states, n_actions), dtype=float)\n",
        "\n",
        "D = []\n",
        "reward = []\n",
        "\n",
        "pbar = tqdm(range(K))\n",
        "for k in pbar:\n",
        "    # In lecture, we ran a \"full\" value iteration here until the Q function stabilizes.\n",
        "    # However, running the \"full\" value iteration every episode is computationally inefficient.\n",
        "    # Instead, run a \"single\" value iteration step to update the Q function once.\n",
        "\n",
        "    # TODO: compute transition probability using the dataset D\n",
        "    # TODO: run a single value iteration to update the Q value function.\n",
        "    policy = Q.argmax(axis=1)\n",
        "\n",
        "    epsilon = get_epsilon(k)\n",
        "    trajectory = run_one_episode(env, H, policy, epsilon)\n",
        "    D.extend(trajectory)\n",
        "    ret = sum(r for _, _, r, _ in trajectory)\n",
        "    reward.append(ret)\n",
        "    pbar.set_postfix({\n",
        "        \"reward\": ret,\n",
        "        \"epsilon\": epsilon,\n",
        "    })\n",
        "\n",
        "# Assuming episode_length list is available from previous execution\n",
        "episode_numbers = range(len(reward))\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "ax1.plot(episode_numbers, reward, color='blue', label='Reward')\n",
        "ax1.set_xlabel('Episode Number')\n",
        "ax1.set_ylabel('Reward', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# Create a second y-axis for epsilon\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(episode_numbers, [get_epsilon(k) for k in range(K)], color='red', linestyle='--', label='Epsilon')\n",
        "ax2.set_ylabel('Epsilon Value', color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "plt.title('Reward and Epsilon vs. Episode Number')\n",
        "fig.tight_layout() # Adjust layout to prevent overlapping labels\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MgcKRqhuQmHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is code for testing the policy learned. It saves the animation into a gif file and downloads it onto your computer."
      ],
      "metadata": {
        "id": "kPA80I-Pb9tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "frames = []\n",
        "\n",
        "obs, info = env.reset(seed=4)\n",
        "for step in range(200):\n",
        "    action = Q.argmax(axis=1)[obs]\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    env.render()\n",
        "\n",
        "    fig = env.fig\n",
        "    fig.canvas.draw()\n",
        "    buf = np.asarray(fig.canvas.buffer_rgba()).copy()\n",
        "    frame = buf[:, :, :3]\n",
        "    frames.append(frame)\n",
        "\n",
        "gif_path = \"trajectory.gif\"\n",
        "imageio.mimsave(gif_path, frames, fps=16)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"trajectory.gif\")"
      ],
      "metadata": {
        "id": "vdbgsmyPYqxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to modify the value iteration algorithm so that it learns a good policy. You may need to adjust the number of episodes, the episode length, and the schedule for decaying ε. The resulting policy should be able to stabilize the inverted pendulum. Please submit the GIF file for your homework; the code does not need to be submitted."
      ],
      "metadata": {
        "id": "1pHLhxsyhfl3"
      }
    }
  ]
}